import torch
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision import transforms
import os
import numpy as np

class Load_Dataset(Dataset):
    def __init__(self, dataset, dataset_configs):
        super().__init__()
        self.num_channels = dataset_configs.input_channels

        # Load samples
        x_data = dataset["samples"]
        
        # Convert to torch tensor
        if isinstance(x_data, np.ndarray):
            x_data = torch.from_numpy(x_data)
                
        # Check samples dimensions.
        # The dimension of the data is expected to be (N, C, L)
        # where N is the #samples, C: #channels, and L is the sequence length
        if len(x_data.shape) == 2:
            x_data = x_data.unsqueeze(1)
        elif len(x_data.shape) == 3 and x_data.shape[1] != self.num_channels:
            x_data = x_data.transpose(1, 2)

        # Convert to torch tensor
        if isinstance(x_data, np.ndarray):
            x_data = torch.from_numpy(x_data)

        # Load labels
        y_data = dataset.get("labels")
        if y_data is not None and isinstance(y_data, np.ndarray):
            y_data = torch.from_numpy(y_data)

        # Normalize data
        if dataset_configs.normalize:
            data_mean = torch.mean(x_data, dim=(0, 2))
            data_std = torch.std(x_data, dim=(0, 2))
            self.transform = transforms.Normalize(mean=data_mean, std=data_std)

        self.x_data = x_data.float()
        self.y_data = y_data.long() if y_data is not None else None
        self.len = x_data.shape[0]

    def __getitem__(self, index):
        x = self.x_data[index]
        if self.transform:
            x = self.transform(self.x_data[index].reshape(self.num_channels, -1, 1)).reshape(self.x_data[index].shape)
        y = self.y_data[index] if self.y_data is not None else None
        return x, y, index

    def __len__(self):
        return self.len

class Load_and_Merge_Datasets(Dataset):
    def __init__(self, datasets, dataset_configs):
        super().__init__()
        self.num_channels = dataset_configs.input_channels
        self.x_data = []
        self.y_data = []

        for dataset in datasets:
            # Load samples
            x_data = dataset["samples"]
            
            # Convert to torch tensor
            if isinstance(x_data, np.ndarray):
                x_data = torch.from_numpy(x_data)
                
            # Check samples dimensions.
            # The dimension of the data is expected to be (N, C, L)
            # where N is the #samples, C: #channels, and L is the sequence length
            if len(x_data.shape) == 2:
                x_data = x_data.unsqueeze(1)
            elif len(x_data.shape) == 3 and x_data.shape[1] != self.num_channels:
                x_data = x_data.transpose(1, 2)

            self.x_data.append(x_data.float())

            # Load labels
            y_data = dataset.get("labels")
            if y_data is not None and isinstance(y_data, np.ndarray):
                y_data = torch.from_numpy(y_data)

            self.y_data.append(y_data.long() if y_data is not None else None)
        
        # Merge x_data
        self.x_data = torch.cat(self.x_data, dim=0)

        # Merge y_data
        if any(y is None for y in self.y_data):
            self.y_data = None
        else:
            self.y_data = torch.cat(self.y_data, dim=0)

        # Normalize data
        if dataset_configs.normalize:
            data_mean = torch.mean(self.x_data, dim=(0, 2))
            data_std = torch.std(self.x_data, dim=(0, 2))
            self.transform = transforms.Normalize(mean=data_mean, std=data_std)

        self.len = self.x_data.shape[0]

    def __getitem__(self, index):
        x = self.x_data[index]
        if self.transform:
            x = self.transform(self.x_data[index].reshape(self.num_channels, -1, 1)).reshape(self.x_data[index].shape)
        y = self.y_data[index] if self.y_data is not None else None
        return x, y, index

    def __len__(self):
        return self.len
    
    
def data_generator(data_path, domain_id, dataset_configs, hparams, dtype, merge_data=False):
    
    if not merge_data:
        # loading dataset file from path
        dataset_file = torch.load(os.path.join(data_path, f"{dtype}_{domain_id}.pt"))

        # Loading datasets
        dataset = Load_Dataset(dataset_file, dataset_configs)
    else:
        # loading dataset file from path
        dataset_files = [torch.load(os.path.join(data_path, f"{dtype}_{did}.pt")) for did in domain_id]

        # Loading datasets
        dataset = Load_and_Merge_Datasets(dataset_files, dataset_configs)

    if dtype == "test":  # you don't need to shuffle or drop last batch while testing
        shuffle = False
        drop_last = False
    else:
        shuffle = dataset_configs.shuffle
        drop_last = dataset_configs.drop_last

    # Dataloaders
    data_loader = torch.utils.data.DataLoader(dataset=dataset,
                                              batch_size=hparams["batch_size"],
                                              shuffle=shuffle,
                                              drop_last=drop_last,
                                              num_workers=0)

    return data_loader
